# app/services/multi_modal_processor.py

import pymupdf as fitz  # Import the library by its official name and alias it to fitz
import base64
import requests
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_google_genai import ChatGoogleGenerativeAI
from app.core.config import settings
from .s3_service import download_file_from_s3
from langchain_core.messages import HumanMessage

try:
    print("ğŸ§  Loading HuggingFace embeddings model...")
    EMBEDDING_MODEL = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2',
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… HuggingFace embeddings model loaded successfully.")
except Exception as e:
    print(f"âŒ CRITICAL ERROR: Failed to load embeddings model: {e}")
    # In a real app, you might want the app to exit if this fails
    EMBEDDING_MODEL = None 

# --- VISION LLM ---
# Also good practice to initialize clients once.
try:
    print("ğŸ‘ï¸  Initializing Google Gemini vision model...")
    # IMPORTANT: Use a stable, public model name for vision
    VISION_LLM = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest", 
        google_api_key=settings.GOOGLE_API_KEY
    )
    print("âœ… Google Gemini vision model initialized successfully.")
except Exception as e:
    print(f"âŒ CRITICAL ERROR: Failed to initialize vision model: {e}")
    VISION_LLM = None

def get_image_caption(image_bytes: bytes, llm: ChatGoogleGenerativeAI) -> str:
    """Uses Gemini Vision to generate a caption for an image."""
    try:
        print(f"ğŸ–¼ï¸  Processing image: {len(image_bytes)} bytes")
        
        # Encode the image bytes to a base64 string
        b64_image = base64.b64encode(image_bytes).decode('utf-8')
        print(f"ğŸ”„ Image encoded to base64, length: {len(b64_image)} characters")
        
        print("ğŸ¤– Calling Gemini Vision API for image description...")
        msg = llm.invoke(
            [
                HumanMessage(
                    content=[
                        {
                            "type": "text",
                            "text": "Describe this financial chart, table, or image from a document in detail. Focus on key data, trends, and conclusions presented. Be factual and objective."
                        },
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{b64_image}"
                        },
                    ]
                )
            ]
        )
        
        caption = msg.content if msg.content else "Description could not be generated for the image."
        print(f"âœ… Image caption generated: {caption[:100]}...")
        return caption
        
    except Exception as e:
        print(f"âŒ Error generating image caption: {e}")
        return "Could not describe the image due to an error."


# try:

print(f"âœ… Embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2")
# except Exception as e:
#     print(f"âš ï¸  Failed to load sentence-transformers model, trying alternative: {e}")
#     # Fallback to a different model if the first one fails
#     embeddings = HuggingFaceEmbeddings(
#         model_name='sentence-transformers/paraphrase-MiniLM-L6-v2',
#         model_kwargs={'device': 'cpu', 'trust_remote_code': True},
#         encode_kwargs={'normalize_embeddings': True}
#     )
#     print(f"âœ… Fallback embeddings model loaded: sentence-transformers/paraphrase-MiniLM-L6-v2")
def smart_chat_ingestion_pipeline(document_id: str, s3_url: str, pinecone_namespace: str):
    """A multi-modal ingestion pipeline that extracts and embeds both text and image descriptions."""
    try:
        print(f"\nğŸš€ ===== STARTING SMART CHAT INGESTION =====")
        print(f"ğŸ“„ Document ID: {document_id}")
        print(f"ğŸ”— S3 URL: {s3_url}")
        print(f"ğŸ“ Pinecone Namespace: {pinecone_namespace}")
        
        print(f"\nâ¬‡ï¸  Downloading file from S3...")
        local_path = download_file_from_s3(s3_url)
        print(f"âœ… File downloaded to: {local_path}")
        
        print(f"\nğŸ”§ Initializing components...")
        # Use a more reliable embeddings model that doesn't have meta tensor issues
        # embeddings = HuggingFaceEmbeddings(
        #     model_name='sentence-transformers/all-MiniLM-L6-v2',
        #     model_kwargs={'device': 'cpu'},
        #     encode_kwargs={'normalize_embeddings': True}
        # )
            
        vision_llm = ChatGoogleGenerativeAI(model="gemma-3-12b-it", google_api_key=settings.GOOGLE_API_KEY)
        print(f"âœ… Vision LLM initialized: gemma-3-12b-it")
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        print(f"âœ… Text splitter configured: chunk_size=1000, overlap=150")
        
        print(f"\nğŸ“– Opening PDF document...")
        doc = fitz.open(local_path)
        total_pages = len(doc)
        print(f"âœ… PDF opened successfully: {total_pages} pages")
        
        all_chunks = []
        text_chunks_count = 0
        image_chunks_count = 0
        
        for page_num, page in enumerate(doc):
            print(f"\nğŸ“„ Processing page {page_num + 1}/{total_pages}...")
            
            # 1. Process Text
            print(f"ğŸ“ Extracting text from page {page_num + 1}...")
            text = page.get_text("text")
            if text.strip():
                text_chunks = text_splitter.create_documents([text], metadatas=[{"page": page_num + 1, "type": "text"}])
                all_chunks.extend(text_chunks)
                text_chunks_count += len(text_chunks)
                print(f"âœ… Text extracted: {len(text_chunks)} chunks, {len(text)} characters")
            else:
                print(f"âš ï¸  No text found on page {page_num + 1}")
            
            # 2. Process Images
            images = page.get_images(full=True)
            print(f"ğŸ–¼ï¸  Found {len(images)} images on page {page_num + 1}")
            
            for img_index, img in enumerate(images):
                print(f"ğŸ–¼ï¸  Processing image {img_index + 1}/{len(images)} on page {page_num + 1}...")
                
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                caption = get_image_caption(image_bytes, vision_llm)
                caption_with_context = f"Context from an image on page {page_num + 1}: {caption}"
                
                image_chunk = text_splitter.create_documents([caption_with_context], metadatas=[{"page": page_num + 1, "type": "image"}])
                all_chunks.extend(image_chunk)
                image_chunks_count += len(image_chunk)
                print(f"âœ… Image processed: {len(image_chunk)} chunks created")

        print(f"\nğŸ“Š PROCESSING SUMMARY:")
        print(f"ğŸ“ Text chunks: {text_chunks_count}")
        print(f"ğŸ–¼ï¸  Image chunks: {image_chunks_count}")
        print(f"ğŸ“¦ Total chunks: {len(all_chunks)}")

        if not all_chunks:
            raise ValueError("Document processing resulted in no text or image chunks.")

        print(f"\nğŸ”„ Upserting embeddings to Pinecone...")
        print(f"ğŸ¯ Target index: cksfinbot")
        print(f"ğŸ“ Target namespace: {pinecone_namespace}")
        
        PineconeVectorStore.from_documents(
            documents=all_chunks,
            embedding=EMBEDDING_MODEL,
            index_name="cksfinbot",
            namespace=pinecone_namespace
        )
        print(f"âœ… SMART CHAT embeddings upserted successfully!")

        print(f"\nğŸ“¡ Sending success webhook to Node.js backend...")
        webhook_response = requests.patch(
            f"{settings.NODE_WEBHOOK_URL}/api/v1/documents/{document_id}/status/webhook", 
            json={"status": "processed"}, 
            timeout=100
        )
        print(f"âœ… Webhook sent successfully: {webhook_response.status_code}")
        
        print(f"\nğŸ‰ ===== DOCUMENT PROCESSING COMPLETED =====")
        print(f"ğŸ“„ Document ID: {document_id}")
        print(f"âœ… Status: PROCESSED")

    except Exception as e:
        print(f"\nâŒ ===== DOCUMENT PROCESSING FAILED =====")
        print(f"ğŸ“„ Document ID: {document_id}")
        print(f"âŒ Error: {str(e)}")
        print(f"ğŸ“¡ Sending failure webhook...")
        
        try:
            webhook_response = requests.patch(
                f"{settings.NODE_WEBHOOK_URL}/api/v1/documents/{document_id}/status/webhook", 
                json={"status": "failed", "errorMessage": str(e)}, 
                timeout=100
            )
            print(f"âœ… Failure webhook sent: {webhook_response.status_code}")
        except Exception as webhook_error:
            print(f"âŒ Failed to send webhook: {webhook_error}")
        
        print(f"âŒ ===== PROCESSING ENDED WITH ERROR =====")